---
layout: post
title: "Blog Post 5"
date: 2021-11-12 22:00:00
---

# Image Classification in Tensorflow


In this blog post, we are going to learn how to construct a machine learning algorithm that classifies images of cats and dogs using `Tensorflow`. Before we get started, we need to load necessary packages. 

For training, validation, and testing, we would use a sample data set provided by the TensorFlow team that contains labeled images of cats and dogs.

## §1.Load Packages and Obtain Data


```python
import os
from tensorflow.keras import utils 
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np
import collections

```


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)

```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.


> ## Explore Our Data Set 
To have a sense of what our data set looks like, we would write a `two_row_visualization()` function that displays labeled pictures of dogs and cats.


```python
def two_row_visualization(train_dataset):
  """
  This function shows 
  three random labeled images of cats in the 1st row
  And three random labeled images of dogs in the 2nd row.
  """
  # get the class names
  class_names = train_dataset.class_names

  plt.figure(figsize=(10, 10))
  # retrieve one batch (32 images with labels) from the training data
  for images, labels in train_dataset.take(1):
    # keep track of indices of the images
    idx_cat = []
    idx_dog = []

    # append the label if it is cat and hold 3 labels as max 
    for i in range(len(labels)):
      if len(idx_cat) < 3:
        if class_names[labels[i]] == "cats":
                       idx_cat.append(i)
    
    # append the label if it is dog and hold 3 labels as max 
      if len(idx_dog) < 3:
        if class_names[labels[i]] == "dogs":
                       idx_dog.append(i)

  idx = idx_cat + idx_dog # concatenate the two indices 

  for i in range(6):
    ax = plt.subplot(2, 3, i + 1)
    plt.imshow(images[idx[i]].numpy().astype("uint8"))
    plt.title(class_names[labels[idx[i]]])      
    plt.axis("off")  


```


```python
two_row_visualization(train_dataset)
```


    
![output_7_0.png](/images/output_7_0.png)
    


The following chunk of code is the technical code related to rapidly reading data.


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

> ## Check Label Frequencies
The following line of code will create an *iterator* called `labels`.


```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```


```python
# compute the number of images in the training data  with label 0 ("cat") and label 1 ("dog")
# using the Counter method which stores dictionary keys and their counts as corresponding dictionary values
collections.Counter(list(labels_iterator))
```




    Counter({0: 1000, 1: 1000})



The *baseline* machine learning model is the model that always guesses the most frequent label. In our case, since we have 50% numbers of *label 0* and 50% numbers of *label 1* (i.e., the two labels have the same frequency), the baseline line model should have an accuracy of 50%.

Our models should obtain accuracy higher than baseline in order to be considered good data science achievements!

## §2. First Model 
In this section, we will create a `tf.keras.Sequential` model using some of the layers such as `Conv2D`, `MaxPooling2D`, `Flatten`, `Dense`, and `Dropout` layers.


```python
model1 = models.Sequential([
    # beginning (input)
    # starts with the Conv2D layer with 32 kernels and shape 3*3 
    # passes a non-linear transformation called relu
    # and specifies the input shape (160x160 pixels and three color "channels")                      
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),

    # hidden layers
    # reduce the number of parameters at each step
    layers.MaxPooling2D((2, 2)),

    # add more layers to make the model "deeper"
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    # Dropout layer deals with overfitting by ensuring that the model doesn't fit too close
    layers.Dropout(0.2),

    # flatten the data from 2d to 1d in order to pass it through the final Dense layers
    layers.Flatten(),

    # end (output)
    # Dense layers form the prediction
    layers.Dense(2) # we have 2 classes
])
```


After we have constructed the model, we can now train our model by using `model.compile(optimizer, loss, metrics)`.


```python
model1.compile(optimizer='adam',
              # apply softmax by specifying from_logits=True 
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']) # we want to see how accuracy changes

```

Now we can check how it works and plot the history of the accuracy on both the training and validation sets.


```python
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 81ms/step - loss: 10.0844 - accuracy: 0.5360 - val_loss: 0.6986 - val_accuracy: 0.5458
    Epoch 2/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6735 - accuracy: 0.5720 - val_loss: 0.6874 - val_accuracy: 0.5470
    Epoch 3/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6238 - accuracy: 0.6520 - val_loss: 0.6903 - val_accuracy: 0.5817
    Epoch 4/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5900 - accuracy: 0.6800 - val_loss: 0.7213 - val_accuracy: 0.5631
    Epoch 5/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5406 - accuracy: 0.7290 - val_loss: 0.7007 - val_accuracy: 0.6176
    Epoch 6/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.4698 - accuracy: 0.7600 - val_loss: 0.8192 - val_accuracy: 0.6213
    Epoch 7/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.4212 - accuracy: 0.7955 - val_loss: 0.8043 - val_accuracy: 0.6448
    Epoch 8/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.3966 - accuracy: 0.8095 - val_loss: 0.8699 - val_accuracy: 0.6225
    Epoch 9/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.3554 - accuracy: 0.8445 - val_loss: 0.8810 - val_accuracy: 0.6374
    Epoch 10/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.3388 - accuracy: 0.8485 - val_loss: 0.9265 - val_accuracy: 0.6374
    Epoch 11/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.3235 - accuracy: 0.8575 - val_loss: 1.0878 - val_accuracy: 0.6213
    Epoch 12/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.3068 - accuracy: 0.8705 - val_loss: 1.0411 - val_accuracy: 0.6089
    Epoch 13/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.3587 - accuracy: 0.8415 - val_loss: 1.0178 - val_accuracy: 0.6324
    Epoch 14/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.2536 - accuracy: 0.8845 - val_loss: 1.3270 - val_accuracy: 0.6411
    Epoch 15/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.2268 - accuracy: 0.9070 - val_loss: 1.3200 - val_accuracy: 0.6448
    Epoch 16/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.1851 - accuracy: 0.9260 - val_loss: 1.4097 - val_accuracy: 0.6547
    Epoch 17/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.2024 - accuracy: 0.9175 - val_loss: 1.3225 - val_accuracy: 0.6572
    Epoch 18/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.2097 - accuracy: 0.9185 - val_loss: 1.4152 - val_accuracy: 0.6200
    Epoch 19/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.1772 - accuracy: 0.9335 - val_loss: 1.5538 - val_accuracy: 0.6609
    Epoch 20/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.1619 - accuracy: 0.9320 - val_loss: 1.5055 - val_accuracy: 0.6436



```python
# plot the history accuracy of training and validation data sets
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f7c005a3850>




    
![output_20_1.png](/images/output_20_1.png)
    


When constructing the model1, I experimented with the kernel size of `layers.Conv2D` and tried different matrix size for `layers.maxpooling2D`. I've also tried adding more layers to see the performance.

- **The validation accuracy of my model stabilized between 62% and 66% during training.**
- Compared to the 50% baseline accuracy, my model is about 12 to 16 percent better.
- In our case, since the training accuracy is much higher than the validation accuracy, `model1` does display overfitting. 

## §3. Model with Data Augmentation
Now we’re going to add some *data augmentation layers* to our model. Data augmentation means including modified copies of the same image in the training set. The purpose of data augmentation is help our model learn invariant features of our input images. 

> Make a plot of the original image (the first image) and a few copies to which `RandomFlip()` has been applied.


```python
RandomFlip = tf.keras.layers.RandomFlip(mode = "horizontal_and_vertical")
```


```python
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  for i in range(4):
    ax = plt.subplot(1, 4, i + 1)
    # plot the first image as the original image
    if i == 0 :
      plt.imshow(image[0].numpy().astype('uint8'))
      plt.axis('off')
    else:
      augmented_image = RandomFlip(tf.expand_dims(image[0], 0))
      plt.imshow(augmented_image[0] / 255)
      plt.axis('off')
```


    
![output_25_0.png](/images/output_25_0.png)
    


> make a plot of both the original image (the first image) and a few copies to which `RandomRotation()` has been applied.


```python
RandomRotation = tf.keras.layers.RandomRotation(0.2)
```


```python
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  #first_image = image[0]
  for i in range(4):
    ax = plt.subplot(1, 4, i + 1)
    # plot the first image as the original image
    if i == 0 :
      plt.imshow(image[0].numpy().astype('uint8'))
      plt.axis('off')
    else:
      augmented_image = RandomRotation(tf.expand_dims(image[0], 0))
      plt.imshow(augmented_image[0] / 255)
      plt.axis('off')
```


    
![output_28_0.png](/images/output_28_0.png)
    


Now we would apply the `RandomFlip()` and `RandomRotation()` layers and see how this new model performs.


```python
model2 = tf.keras.models.Sequential([
                                     
    # apply the data augmentation layers                                 
    layers.RandomFlip(),
    layers.RandomRotation(0.2),                                 

    # the following layers are the same as in model1                  
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.2),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),

    layers.Dense(64, activation='relu'),
    layers.Dense(2) # we have 2 classes
])
```


```python
# train the model2 on our data set
model2.compile(optimizer='adam',
              # apply softmax by specifying from_logits=True 
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']) # we want to see how accuracy changes

history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)

```

    Epoch 1/20
    63/63 [==============================] - 7s 81ms/step - loss: 32.2756 - accuracy: 0.5015 - val_loss: 0.7240 - val_accuracy: 0.4975
    Epoch 2/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6914 - accuracy: 0.5215 - val_loss: 0.6903 - val_accuracy: 0.5408
    Epoch 3/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6881 - accuracy: 0.5390 - val_loss: 0.6822 - val_accuracy: 0.5941
    Epoch 4/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6851 - accuracy: 0.5470 - val_loss: 0.6787 - val_accuracy: 0.5854
    Epoch 5/20
    63/63 [==============================] - 7s 102ms/step - loss: 0.6833 - accuracy: 0.5700 - val_loss: 0.6811 - val_accuracy: 0.5854
    Epoch 6/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6811 - accuracy: 0.5550 - val_loss: 0.7562 - val_accuracy: 0.5483
    Epoch 7/20
    63/63 [==============================] - 5s 83ms/step - loss: 0.6751 - accuracy: 0.5880 - val_loss: 0.6790 - val_accuracy: 0.5569
    Epoch 8/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6759 - accuracy: 0.5770 - val_loss: 0.6837 - val_accuracy: 0.5965
    Epoch 9/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6723 - accuracy: 0.5900 - val_loss: 0.6752 - val_accuracy: 0.5941
    Epoch 10/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6678 - accuracy: 0.5945 - val_loss: 0.6775 - val_accuracy: 0.5866
    Epoch 11/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6642 - accuracy: 0.6130 - val_loss: 0.6616 - val_accuracy: 0.6238
    Epoch 12/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6666 - accuracy: 0.6065 - val_loss: 0.6496 - val_accuracy: 0.6200
    Epoch 13/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6731 - accuracy: 0.5910 - val_loss: 0.6880 - val_accuracy: 0.5594
    Epoch 14/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6677 - accuracy: 0.6040 - val_loss: 0.6639 - val_accuracy: 0.6126
    Epoch 15/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6664 - accuracy: 0.6095 - val_loss: 0.6688 - val_accuracy: 0.5891
    Epoch 16/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6757 - accuracy: 0.5975 - val_loss: 0.6816 - val_accuracy: 0.5359
    Epoch 17/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6740 - accuracy: 0.5790 - val_loss: 0.6597 - val_accuracy: 0.6250
    Epoch 18/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6639 - accuracy: 0.6150 - val_loss: 0.6682 - val_accuracy: 0.6089
    Epoch 19/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6579 - accuracy: 0.6145 - val_loss: 0.6711 - val_accuracy: 0.6015
    Epoch 20/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6626 - accuracy: 0.5970 - val_loss: 0.6618 - val_accuracy: 0.6238



```python
# plot the history accuracy of training and validation data sets
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f7c0008cc10>




    
![output_32_1.png](/images/output_32_1.png)
    


- **The validation accuracy of model2 stabilized between 60% and 62% during training.**
- Compared to the accuracy of model1 (62%-66%), `model2` is about 2% worse.
- Since the training accuracy is about the same as the validation accuracy, `model2` does **not** display overfitting. 

## §4. Data Preprocessing
Sometimes it can be helpful to make simple transformations to the input data so that we can spend more of our training energy handling actual signal in the data and less energy having the weights adjust to the data scale. For example, we can normalize RGB values (originally between 0 and 255) of our original data to 0 and 1, so our model could train faster.

The following code will create a preprocessing layer called `preprocessor` which we can slot into our model pipeline.


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

Now we will incorporate the preprocessing layer into our `model3` and see how it does.


```python
model3 = tf.keras.models.Sequential([

    # apply the preprocessor to get faster training
    preprocessor,

    # apply the data augmentation layers                                 
    layers.RandomFlip(),
    layers.RandomRotation(0.2),                                 

    # the following layers are the same as in model1                  
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.2),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),

    layers.Dense(64, activation='relu'),
    layers.Dense(2) # we have 2 classes
])
```


```python
# train the model3 on our data set
model3.compile(optimizer='adam',
              # apply softmax by specifying from_logits=True 
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']) # we want to see how accuracy changes

history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)

```

    Epoch 1/20
    63/63 [==============================] - 7s 85ms/step - loss: 0.7059 - accuracy: 0.5380 - val_loss: 0.6712 - val_accuracy: 0.5408
    Epoch 2/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6640 - accuracy: 0.5715 - val_loss: 0.6580 - val_accuracy: 0.5730
    Epoch 3/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6514 - accuracy: 0.6070 - val_loss: 0.6502 - val_accuracy: 0.6262
    Epoch 4/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6368 - accuracy: 0.6190 - val_loss: 0.6522 - val_accuracy: 0.5842
    Epoch 5/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6270 - accuracy: 0.6360 - val_loss: 0.6418 - val_accuracy: 0.6213
    Epoch 6/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6060 - accuracy: 0.6700 - val_loss: 0.6768 - val_accuracy: 0.5594
    Epoch 7/20
    63/63 [==============================] - 5s 83ms/step - loss: 0.6038 - accuracy: 0.6815 - val_loss: 0.5973 - val_accuracy: 0.6745
    Epoch 8/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5831 - accuracy: 0.6900 - val_loss: 0.6157 - val_accuracy: 0.6498
    Epoch 9/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.5690 - accuracy: 0.6955 - val_loss: 0.5901 - val_accuracy: 0.6671
    Epoch 10/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5687 - accuracy: 0.6960 - val_loss: 0.5976 - val_accuracy: 0.6807
    Epoch 11/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5557 - accuracy: 0.7095 - val_loss: 0.5673 - val_accuracy: 0.7153
    Epoch 12/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.5530 - accuracy: 0.7155 - val_loss: 0.5800 - val_accuracy: 0.6733
    Epoch 13/20
    63/63 [==============================] - 5s 83ms/step - loss: 0.5498 - accuracy: 0.7200 - val_loss: 0.5615 - val_accuracy: 0.7141
    Epoch 14/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.5435 - accuracy: 0.7135 - val_loss: 0.5662 - val_accuracy: 0.7079
    Epoch 15/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5415 - accuracy: 0.7210 - val_loss: 0.5872 - val_accuracy: 0.6844
    Epoch 16/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5273 - accuracy: 0.7380 - val_loss: 0.5454 - val_accuracy: 0.7215
    Epoch 17/20
    63/63 [==============================] - 5s 83ms/step - loss: 0.5235 - accuracy: 0.7380 - val_loss: 0.5842 - val_accuracy: 0.7153
    Epoch 18/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5231 - accuracy: 0.7380 - val_loss: 0.5457 - val_accuracy: 0.7178
    Epoch 19/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.5117 - accuracy: 0.7445 - val_loss: 0.5717 - val_accuracy: 0.6894
    Epoch 20/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.5087 - accuracy: 0.7305 - val_loss: 0.5380 - val_accuracy: 0.7203



```python
# plot the history accuracy of training and validation data sets
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f7b89782dd0>




    
![output_39_1.png](/images/output_39_1.png)
    


- **The validation accuracy of model3 stabilized around 72% during training.**
- Compared to the accuracy of model1 (62%-66%), `model3` is 6% to 10% better.
- Since the training accuracy is about the same as the validation accuracy, `model3` does **not** display overfitting. 

## §5. Transfer Learning
Sometimes we can benefit by adopting a pre-existing model and then build our model on this. This process is called transfer learning. To do this, we need to first access a pre-existing “base model”, incorporate it into a full model for our current task, and then train that model.

The following code downloads `MobileNetV2` and configures it as a layer that can be included in our model.


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5
    9412608/9406464 [==============================] - 0s 0us/step
    9420800/9406464 [==============================] - 0s 0us/step


Now we are able to create a model called `model4` that uses `MobileNetV2`.


```python
model4 = tf.keras.models.Sequential([

    # apply the preprocessor to get faster training
    preprocessor,

    # apply the data augmentation layers                                 
    layers.RandomFlip(),
    layers.RandomRotation(0.2),    

    # the base model layer constructed above
    base_model_layer,                           

    # some additional layers                
    layers.GlobalMaxPooling2D(),  
    layers.Dropout(0.2),
    layers.Flatten(),

    layers.Dense(64, activation='relu'),
    layers.Dense(2) # perform the classification
])
```

Now we would like to use the `model4.summary()` method to see the hidden mechanics in the `base_model_layer`.


```python
model4.summary()
```

    Model: "sequential_3"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     random_flip_3 (RandomFlip)  (None, 160, 160, 3)       0         
                                                                     
     random_rotation_3 (RandomRo  (None, 160, 160, 3)      0         
     tation)                                                         
                                                                     
     model_1 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     global_max_pooling2d (Globa  (None, 1280)             0         
     lMaxPooling2D)                                                  
                                                                     
     dropout_3 (Dropout)         (None, 1280)              0         
                                                                     
     flatten_3 (Flatten)         (None, 1280)              0         
                                                                     
     dense_5 (Dense)             (None, 64)                81984     
                                                                     
     dense_6 (Dense)             (None, 2)                 130       
                                                                     
    =================================================================
    Total params: 2,340,098
    Trainable params: 82,114
    Non-trainable params: 2,257,984
    _________________________________________________________________


We can observe that there are **82,114** trainable parameters in this model.


```python
# train the model4 on our data set
model4.compile(optimizer='adam',
              # apply softmax by specifying from_logits=True 
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']) # we want to see how accuracy changes

history = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 12s 116ms/step - loss: 0.6484 - accuracy: 0.7975 - val_loss: 0.0798 - val_accuracy: 0.9703
    Epoch 2/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.2749 - accuracy: 0.8860 - val_loss: 0.0838 - val_accuracy: 0.9703
    Epoch 3/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.2540 - accuracy: 0.8930 - val_loss: 0.0922 - val_accuracy: 0.9592
    Epoch 4/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.2084 - accuracy: 0.9090 - val_loss: 0.0750 - val_accuracy: 0.9715
    Epoch 5/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.2068 - accuracy: 0.9145 - val_loss: 0.0726 - val_accuracy: 0.9777
    Epoch 6/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.2170 - accuracy: 0.9075 - val_loss: 0.0784 - val_accuracy: 0.9728
    Epoch 7/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1924 - accuracy: 0.9200 - val_loss: 0.0727 - val_accuracy: 0.9740
    Epoch 8/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.2134 - accuracy: 0.9065 - val_loss: 0.0663 - val_accuracy: 0.9790
    Epoch 9/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.1747 - accuracy: 0.9290 - val_loss: 0.0623 - val_accuracy: 0.9765
    Epoch 10/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.1769 - accuracy: 0.9290 - val_loss: 0.0638 - val_accuracy: 0.9777
    Epoch 11/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1861 - accuracy: 0.9215 - val_loss: 0.0592 - val_accuracy: 0.9802
    Epoch 12/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1629 - accuracy: 0.9310 - val_loss: 0.0578 - val_accuracy: 0.9802
    Epoch 13/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.1735 - accuracy: 0.9210 - val_loss: 0.0606 - val_accuracy: 0.9752
    Epoch 14/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.1644 - accuracy: 0.9330 - val_loss: 0.0565 - val_accuracy: 0.9802
    Epoch 15/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.1651 - accuracy: 0.9345 - val_loss: 0.0588 - val_accuracy: 0.9802
    Epoch 16/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.1573 - accuracy: 0.9335 - val_loss: 0.0766 - val_accuracy: 0.9678
    Epoch 17/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1558 - accuracy: 0.9410 - val_loss: 0.0688 - val_accuracy: 0.9740
    Epoch 18/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.1563 - accuracy: 0.9355 - val_loss: 0.0597 - val_accuracy: 0.9752
    Epoch 19/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.1748 - accuracy: 0.9300 - val_loss: 0.0635 - val_accuracy: 0.9752
    Epoch 20/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1582 - accuracy: 0.9335 - val_loss: 0.0461 - val_accuracy: 0.9790



```python
# plot the history accuracy of training and validation data sets
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f7b14109e50>




    
![output_50_1.png](/images/output_50_1.png)
    


- **The validation accuracy of model4 stabilized around 97% during training.**
- `model4` performs better than any of the previous models.
- For the overfitting issue, we can observe that the validation accuracy is even higher than the training accuracy, so `model4` does **not** display overfitting. 

## §6. Score on Test Data
`model4` is my most performant model. Let's see how it evaluates on the `test_dataset`.


```python
eval = model4.evaluate(test_dataset)
print("Test accuracy: ", eval[1])
```

    6/6 [==============================] - 1s 61ms/step - loss: 0.0485 - accuracy: 0.9792
    Test accuracy:  0.9791666865348816


We can see that our test accuracy is almost 98%, which is a pretty decent achievement!
