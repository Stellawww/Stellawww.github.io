---
layout: post
title: "Blog Post 4"
date: 2021-11-05 20:00:00
---

## Spectral Clustering

In this blog post, we will learn how to implement a simple version of the *spectral clustering* algorithm for clustering data points. 

Spectral clustering is useful for identifying meaningful parts of data sets with complex structure. In our case, specifically, we are going to look at how spectral clustering could cluster two crescents that is shown in the plot below:



```python
import numpy as np
from sklearn import datasets
import sklearn.metrics
from sklearn.cluster import KMeans
from matplotlib import pyplot as plt
```


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fb736419210>




    
![output_2_1.png](/images/output_2_1.png)
    


In the following sections, we will derive and implement spectral clustering on this plot.

### Some Notations for the Rest of the Post


- $\mathbf{A}$ refer to matrices (2d arrays of numbers). 
- $\mathbf{v}$ refer to vectors (1d arrays of numbers). 
- $\mathbf{A}\mathbf{B}$ refers to a matrix-matrix product (`A@B`). $\mathbf{A}\mathbf{v}$ refers to a matrix-vector product (`A@v`). 


> ### Part A: Creating the Similarity Matrix 

We would like to construct a *similarity matrix* $\mathbf{A}$. 

- $\mathbf{A}$ should be a matrix (2d np.ndarray) with shape `(n, n)` (note that `n` is the number of data points). 

- It collects all the pairwise distances and filters them according to the parameter `epsilon`. Entry `A[i,j]` should be equal to `1` if `X[i]` is within distance `epsilon` of `X[j]`; Otherwise, the entry is `0`. 

- Also, we want all the diagonal entries to be zero.

To correctly construct the similarity matrix, we are going to use the function `sklearn.metrics.pairwise_distances()` which is built into `sklearn`.

For this part, we chose `epsilon = 0.4`. 


```python
def similarity_matrix(X, epsilon):
    """
    This function constructs a matrix A that contains information for 
    all of the pairwise distances between data points.
    1 means within distance epsilon while 0 means otherwirse.
    """
    
    # create matrix A that contains the pairwise distances
    A = sklearn.metrics.pairwise_distances(X, metric = "euclidean")
    
    # make A[i,j] equal to 1 if X[i] is within distance epsilon of X[j] 
    # 0 otherwise
    A[A < epsilon] = 1
    A[A != 1] = 0
    
    # make all diagonal enteries equal to zero
    np.fill_diagonal(A, 0)
    
    return A 
```


```python
A = similarity_matrix(X, 0.4)
A
```




    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 1., 0.],
           ...,
           [0., 0., 0., ..., 0., 1., 1.],
           [0., 0., 1., ..., 1., 0., 1.],
           [0., 0., 0., ..., 1., 1., 0.]])



> ### Part B: The Binary Norm Cut Objective

The matrix `A` now contains information about which points are near (within distance `epsilon`) which other points. We now want to cluster the data points in `X` by partitioning the rows and columns of `A`. 

Here are some important definitions for the variables that we are going to see in a matrix's *binary norm cut objective*:
- $$d_i = \sum_{j = 1}^n a_{ij}$$ is the $$i$$th row-sum of $$\mathbf{A}$$.
- $C_0$ and $C_1$ are two clusters of the data points. We assume that every data point is in either $C_0$ or $C_1$. 
- `y` specifies the cluster membership. If `y[i] = 1`, then point `i` (i.e., row $i$ of $\mathbf{A}$) is an element of cluster $C_1$.  

The *binary norm cut objective* of a matrix $\mathbf{A}$ is given by the function:

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$ is the *cut* of the clusters $C_0$ and $C_1$. 
- $\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$ is a measure of the size of the cluster. 

We want $N_{\mathbf{A}}(C_0, C_1)$ to satisfy:

1. There are relatively few entries of $\mathbf{A}$ that join both $C_0$ and $C_1$. 
2. Neither $C_0$ and $C_1$ are too small. 

Now let's look at the cut term and the volumn term in the binary normcut objective function separately. 

#### B.1 The Cut Term

The cut term $\mathbf{cut}(C_0, C_1)$ is the number of nonzero entries in $\mathbf{A}$, which are points in cluster $C_0$ that is "epsilon"-distance within points in cluster $C_1$. This term should be small since we don't want points in $C_0$ to be very close to points in $C_1$. 

Now we want to write a `cut(A, y)` function that computes the cut term.




```python
def cut(A, y):
    """
    This function computes the cut term by adding entries Aij for
    each pair of points (i,j) in clusters C0 and C1
    """
    # initialize the cut
    cut = 0 
    
    # points in the 1st cluster
    c0 = np.where(y==0)[0]
    # points in the 2nd cluster
    c1 = np.where(y==1)[0]
    
    for i in c0:
        for j in c1:
            cut += A[i, j]
    
    return cut
    
```

To test our cut objective, we compare the cut term for true culusters `y` with the randomly generated clusters:


```python
# the cut objective for true clusters y
true_cut = cut(A, y)

# create two random clusters labled either 0 or 1 with length 200
y_random = np.random.randint(2, size = 200)
random_cut = cut(A, y_random)
```


```python
true_cut
```




    13.0




```python
random_cut
```




    1092.0



We can observe that the cut term for true clusters is less than that of the random clusters, which means that the cut objective indeed favors the true clusters over the random ones. 

#### B.2 The Volume Term 

Moving to the second term in our norm cut objective, the *volume term* of cluster $C_0$ is a measure of how "big" cluster $C_0$ is. If cluster $C_0$ is small, then $\frac{1}{\mathbf{vol}(C_0)}$ will be large, but we do not want such a high value. This means that we do not want $C_0$ and $C_1$ to have small sizes.

Now we will construct a `vols(A,y)` function that computes the volumes of $C_0$ and $C_1$ and return a tuple as the result.



```python
def vols(A, Y):
    """
    This function computes the volumes of C0 and C1 
    and return their volumnes as a tuple
    """
    # points in the 1st cluster
    c0_idx = np.where(y==0)[0]
    # points in the 2nd cluster
    c1_idx = np.where(y==1)[0]
    
    c0 = A[c0_idx, :] # extract all columns for C0
    c1 = A[c1_idx, :] # extract all columns for C1
    
    vol_0 = np.sum(np.sum(c0, axis = 1)) # size of cluster 0
    vol_1 = np.sum(np.sum(c1, axis = 1)) # size of cluster 1
    
    return tuple([vol_0, vol_1])
```

Then we can finally compute the normcut objective!


```python
def normcut(A, y):
    """
    This function computes the binary normcut objective of matrix A
    """
    vol_0 = vols(A, y)[0]
    vol_1 = vols(A, y)[1]
    
    normcut = cut(A, y) * (1/vol_0 + 1/vol_1)
    
    return normcut
    
```

Now, compare the `normcut` objective using both the true labels `y` and the fake labels you generated above. What do you observe about the normcut for the true labels when compared to the normcut for the fake labels? 


```python
# normcut for the true labels y
normcut_true = normcut(A, y)
normcut_true
```




    0.011518412331615225




```python
# normcut for the fake labels
normcut_fake = normcut(A, y_random)
normcut_fake
```




    0.9675466358556789



We can observe that normcut for the true labels are smaller than that for the fake labels

> ### Part C: Transform the minimization problem for the normcut objective 

Let's consider a math trick that help us solve the clustering problem (i.e., minimize the nromcut objective function):

Define a new vector $\mathbf{z} \in \mathbb{R}^n$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of the elements of $\mathbf{z}$ contain all the information from $\mathbf{y}$: if $i$ is in cluster $C_0$, then $y_i = 0$ and $z_i > 0$. 

Also, we are given that:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $\mathbf{D}$ is the diagonal matrix with nonzero entries $d_{ii} = d_i$, and where $d_i = \sum_{j = 1}^n a_i$ is the degree (row-sum) from before.  


#### C.1 The `transform(A,y)` function


```python
def transform(A, y):
    """
    This function computes the z vector according to the above formula
    """
    # initialize a vector called z in R^n 
    z = np.zeros(y.shape[0])
    
    # assign value to z's entries where points are in cluster 0
    z[np.where(y==0)[0]] = 1/vols(A, y)[0]
    
    # assign value to z's entries where points are in cluster 1
    z[np.where(y==1)[0]] = (-1)/vols(A, y)[1]
    
    return z
    
```

#### C.2 Verify the above equation for normcut objective


```python
# obtain vector z
z = transform(A, y)

# create diagonal matrix 
D = np.zeros(shape = (n,n))
d = np.sum(A, axis = 1) # the row sum
np.fill_diagonal(D, d) # populate nonzero entries of D with d

# matrix multiplication according to the formula
N_a = (z.T @ (D - A) @ z)/(z.T @ D @ z)

# check if the two values are indistinguishable in a computer's sense
np.isclose(N_a, normcut_true)
```




    True



Thus, we are able to verify that the new formula for normcut given in this section is a valid approximation for the true value for normcut.

#### C.3 Check whether z contains roughly as many positive as negative entries


```python
one = np.ones(200)
z.T @ D @ one
```




    0.0



We successfully checked that the identity $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$.

> ### Part D

In Part C, we have shown that the problem of minimizing the normcut objective can be approximated by minimizing the function:

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$.

We can incorporate this condition in our optimization and transform our problem into minimizing the `orth_obj` function which is given by the prompt.



```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

We can use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $\mathbf{z}$.


```python
from scipy.optimize import minimize

# the optimal clustering 
z_min = minimize(orth_obj, z, method = "nelder-mead").x 
```

Now we have found `z_min` which is the solution for our optimization problem. It clusters the points into two labels as well as possible.

> ### Part E: Plot the Clusters

In this part, we plot our original data according to the sign of `z_min`, which contains information about which cluster the points are in. We use one color to represent points such that `z_min[i] < 0` and the other color to represent points such that `z_min[i] >= 0`



```python
z_min[z_min >= 0] = 0
z_min[z_min < 0] = 1

plt.scatter(X[:,0], X[:,1], c = z_min)
```




    <matplotlib.collections.PathCollection at 0x7fb7364a77d0>




    
![output_37_1.png](/images/output_37_1.png)
    


Awesome! It looks like our attempt to label the clusters is good.

> ### Part F: Solve the Optimization Problem using Eigenvalues and Eigenvectors 

Although we did quite well in Part E, there is a more efficient way to solve this problem by using eigenvalues and eigenvectors! 

We are given that this spectral clustering problem is equivalent to solve the standard eigenvalue problem: 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

We want to find the vector $\mathbf{z}$ which is the eigenvector with the *second*-smallest eigenvalue of the matrix $\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$. 



```python
# construct the Laplacian matrix
L = np.linalg.pinv(D) @ (D - A) 
# note that we use psuedo-inverse here because 
# it enables us to get the inverse of a singular matrix

# compute the eigenvalues and eigenvectors of L
eigenvals_sorted = np.linalg.eig(L)[0].argsort() # get indices of sorted eigenvals
eigenvects = np.linalg.eig(L)[1]

# eigenvectors corresponding to its second-smallest eigenvals
z_eig = eigenvects[:, eigenvals_sorted][:, 1]

# plot the data
z_eig[z_eig >= 0] = 0
z_eig[z_eig < 0] = 1

plt.scatter(X[:,0], X[:,1], c = z_eig)
```




    <matplotlib.collections.PathCollection at 0x7fb737009b50>




    
![output_40_1.png](/images/output_40_1.png)
    


We can see that this method also works pretty well at clustering the points with only one point misclustered.

> ### Part G: Synthesize Our Results!!!

In this part, we will write a function called `spectral_clustering(X, epsilon)` , which takes in the input data `X` and the distance parameter `epsilon`, that performs spectral clustering. It will return an array of binary labels indicating whether data point `i` is in cluster `0` or cluster `1`. 



```python
def spectral_clustering(X, epsilon):
    """
    Purpose
    ----------
    This fuction uses eigenvalues and eigenvectors to implement
    spectral clustering on given data based on distance parameter epsilon
    
    Assumptions
    ----------
    We approximate the spectral clustering problem by a standard 
    eigenvalue problem in Part F
    
    Parameters
    ----------
    first : X
        the data which is a 2d np.ndarray with shape (200, 200)
    second : epsilon
        the distance threshold that determines if two points are in the same cluster
    
    Return
    -------
    array
        binary labels of points obtained by the spectral clustering process
    """
    # create the similarity matrix
    A = similarity_matrix(X, epsilon)
    
    # create the diagonal matrix 
    D = np.zeros(shape = (n,n))
    d = np.sum(A, axis = 1) # the row sum
    np.fill_diagonal(D, d) # populate nonzero entries of D with d
    
    # construct the Laplacian matrix
    L = np.linalg.pinv(D) @ (D - A) 

    # compute the eigenvalues and eigenvectors of L
    eigenvals_sorted = np.linalg.eig(L)[0].argsort() # get indices of sorted eigenvals
    eigenvects = np.linalg.eig(L)[1]

    # eigenvectors corresponding to its second-smallest eigenvals
    z_eig = eigenvects[:, eigenvals_sorted][:, 1]

    # assign point labels
    z_eig[z_eig >= 0] = 0
    z_eig[z_eig < 0] = 1

    return z_eig
    
```


```python
# try our function and plot
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```




    <matplotlib.collections.PathCollection at 0x7fb736fa2790>




    
![output_44_1.png](/images/output_44_1.png)
    


In this attempt, we increase `epsilon` from 0.4 to 0.5, and we can see that there is a slight increase in misclustered points as we relax the distance criteria.

> ### Part H: Testing the `spectral_clustering(X, epsilon)` function on moon clusters


#### H.a 
Let's generate moon-like clusters with `n = 1000`  and `noise = 0.05` and test our `spectral_clustering(X, epsilon)` with `epsilon = 0.4`


```python
# generate the moon-clustered data
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fb738322790>




    
![output_48_1.png](/images/output_48_1.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```




    <matplotlib.collections.PathCollection at 0x7fb738412e90>




    
![output_49_1.png](/images/output_49_1.png)
    


#### H.b
Now let's see what will happen if we increase the `noise` to `0.1`.


```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fb72346f3d0>




    
![output_51_1.png](/images/output_51_1.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```




    <matplotlib.collections.PathCollection at 0x7fb7233caed0>




    
![output_52_1.png](/images/output_52_1.png)
    


As we increase the `noise` from `0.05` to `0.1`, the points become more scattered and the number of misclustered points increases.

#### H.c
Now let's see what will happen if we increase the `noise` to `0.5`.


```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.5, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fb71ed52350>




    
![output_55_1.png](/images/output_55_1.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```




    <matplotlib.collections.PathCollection at 0x7fb71f536b10>




    
![output_56_1.png](/images/output_56_1.png)
    


The data does not even show the two-moons pattern after we increase the `noise` to `0.5`, and our spectral clustering does not work.

#### Part H Takeaway
As we can see in the above three plots, when the `noise` increases, the data become more spread out. At the same time, our spectral clustering becomes less and less accurate.

Thus, the spectral clustering works well only within an appropriate range of `noise`.

> ### Part I: Testing `spectral_clustering(X, epsilon)` function on another data set -- the bull's eye!




```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fb71f536650>




    
![output_60_1.png](/images/output_60_1.png)
    


This dataset has two concentric circles. We can see that the common k-means method does not do well here. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fb71fbef790>




    
![output_62_1.png](/images/output_62_1.png)
    


Now let's experiment on the `epsilon` value and see if our `spectral_clustering(X, epsilon)` would work. 

Let's first try `epsilon = 0.2`:


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.2))
```




    <matplotlib.collections.PathCollection at 0x7fb7203cfc10>




    
![output_64_1.png](/images/output_64_1.png)
    


It seems that `0.2` does not work well for our case.

Now let's try `epsilon = 0.8`:


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.8))
```




    <matplotlib.collections.PathCollection at 0x7fb71fd2f490>




    
![output_66_1.png](/images/output_66_1.png)
    


This is still not what we are looking for.

Then let's try `epsilon = 0.4`:


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```




    <matplotlib.collections.PathCollection at 0x7fb727c54f90>




    
![output_68_1.png](/images/output_68_1.png)
    


Great!!! We finally find the correct value, which is `0.4`. Let's see if other values also work.

`epsilon = 0.3`:


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.3))
```




    <matplotlib.collections.PathCollection at 0x7fb7284ee110>




    
![output_70_1.png](/images/output_70_1.png)
    


`epsilon = 0.5`:


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```




    <matplotlib.collections.PathCollection at 0x7fb727b661d0>




    
![output_72_1.png](/images/output_72_1.png)
    


Awesome! We can observe that `epsilon` values around **0.3 to 0.5** can correctly cluster the bull's eye pattern. Setting an appropriate `epsilon` value is important!
